Converting the data ingestion script for loading data to Postgres to an Airflow DAG

Step by step take the workflow/data pipeline we created in week one (data ingestion pipeline), and put that inside Airflow as a DAG file
	- Steps taken to convert the ingest_data.py script into a DAG file

This will be split into two steps:
	1. Get the csv files
	2. Ingest this csv files into our postgres database

We can do this both in our local/personal computer or in a VM on cloud

Plan:

	1. Start with the docker-compose we had created in week_2/airflow folder
	2. Change the dag mappings
		- Here we would go in the "volumes:" section of the docker-compose file
		- and change ./dags:/opt/airflow/dags to ./dags_local:/opt/airflow/dags
			- the ":" after ./dags means we are mapping the file path /opt/airflow/dags to the ./dags file

	3. Make them run monthly
	4. Create a bash operator, and pass the params: ('execution_date.strftime(\'%Y-#m\')')
	5. Download the data using the code we had already written in previous Airflow activities in week_2
	6. Put the ingest script to Airflow
	7. Modify dependencies
		- Add the ingest script dependencies here
	8. Put the old docker-compose file in the same network

We now create our first empty dag file in our new folder (dags_local): data_ingestion_local.py

We should ideally give our task IDs longer names -- more descriptive of what the task is doing / trying too achieve

When saving file in Airflow via wget, we cannot just save it in default location as that is temporary. 
After the task is run, all the files in that temp folder will deleted. 
So avoid this, we use something like: path_to_local_home/output.csv
The path to local home can be taken via using the OS module

The L flag in "curl -sSL" is for making curl follow a redirect link

Now we want to check where the file was downloaded in our Airflow

	1. We first list out all our docker processes: docker ps
	2. We are interested in the airflow-worker process here
	3. We then enter the following command:
		- docker exec -it airflow-worker-container_id bash
			- by running this command we can now enter that container and check out the downloads and everything
		- wc -l output.csv 
			- command to get the total rows in a csv file

The problem with using output.csv as a name:

	1. Not a good name when we are running multiple DAGs in parallel, and they are downloading files

We want each task to write to its own output file and have unique names. 

Now, we want to get the execution date in YYY-MM format and put that in our URL.

By doing this the URL will get updated with the YYY-MM of the task execution date and download the respective csv file for that month
